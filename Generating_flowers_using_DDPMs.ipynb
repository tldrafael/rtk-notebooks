{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.4/840.4 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.0.1+cu118)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.1)\n",
      "Collecting lightning-utilities>=0.8.0\n",
      "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.24.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.4.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (59.6.0)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.0.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.11.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.9.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->torchmetrics) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.2.1)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.10.1 torchmetrics-1.3.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:52.582929Z",
     "start_time": "2023-02-03T05:53:50.119305Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import base64\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda import amp\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as TF\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "from IPython.display import display, HTML, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# UNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:52.61393Z",
     "start_time": "2023-02-03T05:53:52.585926Z"
    },
    "code_folding": [
     0,
     4,
     29,
     46,
     96,
     105,
     118
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, total_time_steps=1000, time_emb_dims=128, time_emb_dims_exp=512):\n",
    "        super().__init__()\n",
    "\n",
    "        half_dim = time_emb_dims // 2\n",
    "\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "\n",
    "        ts = torch.arange(total_time_steps, dtype=torch.float32)\n",
    "\n",
    "        emb = torch.unsqueeze(ts, dim=-1) * torch.unsqueeze(emb, dim=0)\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "\n",
    "        self.time_blocks = nn.Sequential(\n",
    "            nn.Embedding.from_pretrained(emb),\n",
    "            nn.Linear(in_features=time_emb_dims, out_features=time_emb_dims_exp),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(in_features=time_emb_dims_exp, out_features=time_emb_dims_exp),\n",
    "        )\n",
    "\n",
    "    def forward(self, time):\n",
    "        return self.time_blocks(time)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, channels=64):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups=8, num_channels=channels)\n",
    "        self.mhsa = nn.MultiheadAttention(embed_dim=self.channels, num_heads=4, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, _, H, W = x.shape\n",
    "        h = self.group_norm(x)\n",
    "        h = h.reshape(B, self.channels, H * W).swapaxes(1, 2)  # [B, C, H, W] --> [B, C, H * W] --> [B, H*W, C]\n",
    "        h, _ = self.mhsa(h, h, h)  # [B, H*W, C]\n",
    "        h = h.swapaxes(2, 1).view(B, self.channels, H, W)  # [B, C, H*W] --> [B, C, H, W]\n",
    "        return x + h\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, *, in_channels, out_channels, dropout_rate=0.1, time_emb_dims=512, apply_attention=False):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.act_fn = nn.SiLU()\n",
    "        # Group 1\n",
    "        self.normlize_1 = nn.GroupNorm(num_groups=8, num_channels=self.in_channels)\n",
    "        self.conv_1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        # Group 2 time embedding\n",
    "        self.dense_1 = nn.Linear(in_features=time_emb_dims, out_features=self.out_channels)\n",
    "\n",
    "        # Group 3\n",
    "        self.normlize_2 = nn.GroupNorm(num_groups=8, num_channels=self.out_channels)\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=self.out_channels, out_channels=self.out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        if self.in_channels != self.out_channels:\n",
    "            self.match_input = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=1)\n",
    "        else:\n",
    "            self.match_input = nn.Identity()\n",
    "\n",
    "        if apply_attention:\n",
    "            self.attention = AttentionBlock(channels=self.out_channels)\n",
    "        else:\n",
    "            self.attention = nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # group 1\n",
    "        h = self.act_fn(self.normlize_1(x))\n",
    "        h = self.conv_1(h)\n",
    "\n",
    "        # group 2\n",
    "        # add in timestep embedding\n",
    "        h += self.dense_1(self.act_fn(t))[:, :, None, None]\n",
    "\n",
    "        # group 3\n",
    "        h = self.act_fn(self.normlize_2(h))\n",
    "        h = self.dropout(h)\n",
    "        h = self.conv_2(h)\n",
    "\n",
    "        # Residual and attention\n",
    "        h = h + self.match_input(x)\n",
    "        h = self.attention(h)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.downsample = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        return self.downsample(x)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, *args):\n",
    "        return self.upsample(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=3,\n",
    "        output_channels=3,\n",
    "        num_res_blocks=2,\n",
    "        base_channels=128,\n",
    "        base_channels_multiples=(1, 2, 4, 8),\n",
    "        apply_attention=(False, False, True, False),\n",
    "        dropout_rate=0.1,\n",
    "        time_multiple=4,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        time_emb_dims_exp = base_channels * time_multiple\n",
    "        self.time_embeddings = SinusoidalPositionEmbeddings(time_emb_dims=base_channels, time_emb_dims_exp=time_emb_dims_exp)\n",
    "\n",
    "        self.first = nn.Conv2d(in_channels=input_channels, out_channels=base_channels, kernel_size=3, stride=1, padding=\"same\")\n",
    "\n",
    "        num_resolutions = len(base_channels_multiples)\n",
    "\n",
    "        # Encoder part of the UNet. Dimension reduction.\n",
    "        self.encoder_blocks = nn.ModuleList()\n",
    "        curr_channels = [base_channels]\n",
    "        in_channels = base_channels\n",
    "\n",
    "        for level in range(num_resolutions):\n",
    "            out_channels = base_channels * base_channels_multiples[level]\n",
    "\n",
    "            for _ in range(num_res_blocks):\n",
    "\n",
    "                block = ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=apply_attention[level],\n",
    "                )\n",
    "                self.encoder_blocks.append(block)\n",
    "\n",
    "                in_channels = out_channels\n",
    "                curr_channels.append(in_channels)\n",
    "\n",
    "            if level != (num_resolutions - 1):\n",
    "                self.encoder_blocks.append(DownSample(channels=in_channels))\n",
    "                curr_channels.append(in_channels)\n",
    "\n",
    "        # Bottleneck in between\n",
    "        self.bottleneck_blocks = nn.ModuleList(\n",
    "            (\n",
    "                ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=True,\n",
    "                ),\n",
    "                ResnetBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=in_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=False,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Decoder part of the UNet. Dimension restoration with skip-connections.\n",
    "        self.decoder_blocks = nn.ModuleList()\n",
    "\n",
    "        for level in reversed(range(num_resolutions)):\n",
    "            out_channels = base_channels * base_channels_multiples[level]\n",
    "\n",
    "            for _ in range(num_res_blocks + 1):\n",
    "                encoder_in_channels = curr_channels.pop()\n",
    "                block = ResnetBlock(\n",
    "                    in_channels=encoder_in_channels + in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    time_emb_dims=time_emb_dims_exp,\n",
    "                    apply_attention=apply_attention[level],\n",
    "                )\n",
    "\n",
    "                in_channels = out_channels\n",
    "                self.decoder_blocks.append(block)\n",
    "\n",
    "            if level != 0:\n",
    "                self.decoder_blocks.append(UpSample(in_channels))\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.GroupNorm(num_groups=8, num_channels=in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=output_channels, kernel_size=3, stride=1, padding=\"same\"),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "\n",
    "        time_emb = self.time_embeddings(t)\n",
    "\n",
    "        h = self.first(x)\n",
    "        outs = [h]\n",
    "\n",
    "        for layer in self.encoder_blocks:\n",
    "            h = layer(h, time_emb)\n",
    "            outs.append(h)\n",
    "\n",
    "        for layer in self.bottleneck_blocks:\n",
    "            h = layer(h, time_emb)\n",
    "\n",
    "        for layer in self.decoder_blocks:\n",
    "            if isinstance(layer, ResnetBlock):\n",
    "                out = outs.pop()\n",
    "                h = torch.cat([h, out], dim=1)\n",
    "            h = layer(h, time_emb)\n",
    "\n",
    "        h = self.final(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:52.629929Z",
     "start_time": "2023-02-03T05:53:52.616923Z"
    },
    "code_folding": [
     0,
     6,
     25,
     31,
     39
    ]
   },
   "outputs": [],
   "source": [
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader:\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)\n",
    "\n",
    "def get_default_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def save_images(images, path, **kwargs):\n",
    "    grid = make_grid(images, **kwargs)\n",
    "    ndarr = grid.permute(1, 2, 0).to(\"cpu\").numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(path)\n",
    "    \n",
    "def get(element: torch.Tensor, t: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Get value at index position \"t\" in \"element\" and\n",
    "        reshape it to have the same dimension as a batch of images.\n",
    "    \"\"\"\n",
    "    ele = element.gather(-1, t)\n",
    "    return ele.reshape(-1, 1, 1, 1)\n",
    "\n",
    "def setup_log_directory(config):\n",
    "    '''Log and Model checkpoint directory Setup'''\n",
    "    \n",
    "    if os.path.isdir(config.root_log_dir):\n",
    "        # Get all folders numbers in the root_log_dir\n",
    "        folder_numbers = [int(folder.replace(\"version_\", \"\")) for folder in os.listdir(config.root_log_dir)]\n",
    "        \n",
    "        # Find the latest version number present in the log_dir\n",
    "        last_version_number = max(folder_numbers)\n",
    "\n",
    "        # New version name\n",
    "        version_name = f\"version_{last_version_number + 1}\"\n",
    "\n",
    "    else:\n",
    "        version_name = config.log_dir\n",
    "\n",
    "    # Update the training config default directory \n",
    "    log_dir        = os.path.join(config.root_log_dir,        version_name)\n",
    "    checkpoint_dir = os.path.join(config.root_checkpoint_dir, version_name)\n",
    "\n",
    "    # Create new directory for saving new experiment version\n",
    "    os.makedirs(log_dir,        exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Logging at: {log_dir}\")\n",
    "    print(f\"Model Checkpoint at: {checkpoint_dir}\")\n",
    "    \n",
    "    return log_dir, checkpoint_dir\n",
    "\n",
    "def frames2vid(images, save_path):\n",
    "\n",
    "    WIDTH = images[0].shape[1]\n",
    "    HEIGHT = images[0].shape[0]\n",
    "\n",
    "#     fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#     fourcc = 0\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    video = cv2.VideoWriter(save_path, fourcc, 25, (WIDTH, HEIGHT))\n",
    "\n",
    "    # Appending the images to the video one by one\n",
    "    for image in images:\n",
    "        video.write(image)\n",
    "\n",
    "    # Deallocating memories taken for window creation\n",
    "    # cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "    return \n",
    "\n",
    "def display_gif(gif_path):\n",
    "    b64 = base64.b64encode(open(gif_path,'rb').read()).decode('ascii')\n",
    "    display(HTML(f'<img src=\"data:image/gif;base64,{b64}\" />'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T18:23:28.639407Z",
     "start_time": "2023-02-02T18:23:28.624407Z"
    }
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:53.23984Z",
     "start_time": "2023-02-03T05:53:52.631942Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class BaseConfig:\n",
    "    DEVICE = get_default_device()\n",
    "    DATASET = \"Flowers\" #  \"MNIST\", \"Cifar-10\", \"Cifar-100\", \"Flowers\"\n",
    "    \n",
    "    # For logging inferece images and saving checkpoints.\n",
    "    root_log_dir = os.path.join(\"Logs_Checkpoints\", \"Inference\")\n",
    "    root_checkpoint_dir = os.path.join(\"Logs_Checkpoints\", \"checkpoints\")\n",
    "\n",
    "    # Current log and checkpoint directory.\n",
    "    log_dir = \"version_0\"\n",
    "    checkpoint_dir = \"version_0\"\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    TIMESTEPS = 1000 # Define number of diffusion timesteps\n",
    "    IMG_SHAPE = (1, 32, 32) if BaseConfig.DATASET == \"MNIST\" else (3, 32, 32) \n",
    "    NUM_EPOCHS = 800\n",
    "    BATCH_SIZE = 32\n",
    "    LR = 2e-4\n",
    "    NUM_WORKERS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-02T18:24:36.837306Z",
     "start_time": "2023-02-02T18:24:36.8273Z"
    }
   },
   "source": [
    "# Load Dataset & Build Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:53.25584Z",
     "start_time": "2023-02-03T05:53:53.242828Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_dataset(dataset_name='MNIST'):\n",
    "    transforms = TF.Compose(\n",
    "        [\n",
    "            TF.ToTensor(),\n",
    "            TF.Resize((32, 32), \n",
    "                      interpolation=TF.InterpolationMode.BICUBIC, \n",
    "                      antialias=True),\n",
    "            TF.RandomHorizontalFlip(),\n",
    "            TF.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    if dataset_name.upper() == \"MNIST\":\n",
    "        dataset = datasets.MNIST(root=\"data\", train=True, download=True, transform=transforms)\n",
    "    elif dataset_name == \"Cifar-10\":    \n",
    "        dataset = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transforms)\n",
    "    elif dataset_name == \"Cifar-100\":\n",
    "        dataset = datasets.CIFAR10(root=\"data\", train=True, download=True, transform=transforms)\n",
    "    elif dataset_name == \"Flowers\":\n",
    "        dataset = datasets.ImageFolder(\n",
    "            root=\"/home/rafael/workspace/PX-Matting/notebooks/cache/RTK/flowers\", transform=transforms)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_dataloader(dataset_name='MNIST', \n",
    "                   batch_size=32, \n",
    "                   pin_memory=False, \n",
    "                   shuffle=True, \n",
    "                   num_workers=0, \n",
    "                   device=\"cpu\"\n",
    "                  ):\n",
    "    dataset    = get_dataset(dataset_name=dataset_name)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
    "                            pin_memory=pin_memory, \n",
    "                            num_workers=num_workers, \n",
    "                            shuffle=shuffle\n",
    "                           )\n",
    "    device_dataloader = DeviceDataLoader(dataloader, device)\n",
    "    return device_dataloader\n",
    "\n",
    "def inverse_transform(tensors):\n",
    "    \"\"\"Convert tensors from [-1., 1.] to [0., 255.]\"\"\"\n",
    "    return ((tensors.clamp(-1, 1) + 1.0) / 2.0) * 255.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:53.301809Z",
     "start_time": "2023-02-03T05:53:53.258828Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/flowers-recognition/flowers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBaseConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATASET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m(dataset_name, batch_size, pin_memory, shuffle, num_workers, device)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataloader\u001b[39m(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNIST\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     25\u001b[0m                    batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, \n\u001b[1;32m     26\u001b[0m                    pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m                    device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m                   ):\n\u001b[0;32m---> 31\u001b[0m     dataset    \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m     33\u001b[0m                             pin_memory\u001b[38;5;241m=\u001b[39mpin_memory, \n\u001b[1;32m     34\u001b[0m                             num_workers\u001b[38;5;241m=\u001b[39mnum_workers, \n\u001b[1;32m     35\u001b[0m                             shuffle\u001b[38;5;241m=\u001b[39mshuffle\n\u001b[1;32m     36\u001b[0m                            )\n\u001b[1;32m     37\u001b[0m     device_dataloader \u001b[38;5;241m=\u001b[39m DeviceDataLoader(dataloader, device)\n",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(dataset_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mCIFAR10(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransforms)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlowers\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/flowers-recognition/flowers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py:309\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    303\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    308\u001b[0m ):\n\u001b[0;32m--> 309\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py:144\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    136\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     is_valid_file: Optional[Callable[[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mbool\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[0;32m--> 144\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot, class_to_idx, extensions, is_valid_file)\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py:218\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m    192\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py:40\u001b[0m, in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_classes\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/flowers-recognition/flowers'"
     ]
    }
   ],
   "source": [
    "loader = get_dataloader(\n",
    "    dataset_name=BaseConfig.DATASET,\n",
    "    batch_size=128,\n",
    "    device='cpu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:54.675821Z",
     "start_time": "2023-02-03T05:53:53.30381Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m), facecolor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhite\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b_image, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43mloader\u001b[49m:\n\u001b[1;32m      4\u001b[0m     b_image \u001b[38;5;241m=\u001b[39m inverse_transform(b_image)\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m      5\u001b[0m     grid_img \u001b[38;5;241m=\u001b[39m make_grid(b_image \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m, nrow\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pad_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6), facecolor='white')\n",
    "\n",
    "for b_image, _ in loader:\n",
    "    b_image = inverse_transform(b_image).cpu()\n",
    "    grid_img = make_grid(b_image / 255.0, nrow=16, padding=True, pad_value=1, normalize=True)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis(\"off\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:54.706812Z",
     "start_time": "2023-02-03T05:53:54.678828Z"
    },
    "code_folding": [
     0,
     1,
     15,
     26,
     42
    ],
    "execution": {
     "iopub.execute_input": "2023-02-13T16:02:51.094708Z",
     "iopub.status.busy": "2023-02-13T16:02:51.094319Z",
     "iopub.status.idle": "2023-02-13T16:02:51.107522Z",
     "shell.execute_reply": "2023-02-13T16:02:51.106894Z",
     "shell.execute_reply.started": "2023-02-13T16:02:51.094658Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleDiffusion:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_diffusion_timesteps=1000,\n",
    "        img_shape=(3, 64, 64),\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        self.num_diffusion_timesteps = num_diffusion_timesteps\n",
    "        self.img_shape = img_shape\n",
    "        self.device = device\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        # BETAs & ALPHAs required at different places in the Algorithm.\n",
    "        self.beta  = self.get_betas()\n",
    "        self.alpha = 1 - self.beta\n",
    "        \n",
    "        self_sqrt_beta                       = torch.sqrt(self.beta)\n",
    "        self.alpha_cumulative                = torch.cumprod(self.alpha, dim=0)\n",
    "        self.sqrt_alpha_cumulative           = torch.sqrt(self.alpha_cumulative)\n",
    "        self.one_by_sqrt_alpha               = 1. / torch.sqrt(self.alpha)\n",
    "        self.sqrt_one_minus_alpha_cumulative = torch.sqrt(1 - self.alpha_cumulative)\n",
    "         \n",
    "    def get_betas(self):\n",
    "        \"\"\"linear schedule, proposed in original ddpm paper\"\"\"\n",
    "        scale = 1000 / self.num_diffusion_timesteps\n",
    "        beta_start = scale * 1e-4\n",
    "        beta_end = scale * 0.02\n",
    "        return torch.linspace(\n",
    "            beta_start,\n",
    "            beta_end,\n",
    "            self.num_diffusion_timesteps,\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "        \n",
    "def forward_diffusion(sd: SimpleDiffusion, x0: torch.Tensor, timesteps: torch.Tensor):\n",
    "    eps = torch.randn_like(x0)  # Noise\n",
    "    mean    = get(sd.sqrt_alpha_cumulative, t=timesteps) * x0  # Image scaled\n",
    "    std_dev = get(sd.sqrt_one_minus_alpha_cumulative, t=timesteps) # Noise scaled\n",
    "    sample  = mean + std_dev * eps # scaled inputs * scaled noise\n",
    "\n",
    "    return sample, eps  # return ... , gt noise --> model predicts this)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Forward Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T17:10:21.090589Z",
     "start_time": "2023-02-23T17:10:20.582570Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-13T16:02:51.109795Z",
     "iopub.status.busy": "2023-02-13T16:02:51.108916Z",
     "iopub.status.idle": "2023-02-13T16:02:51.610820Z",
     "shell.execute_reply": "2023-02-13T16:02:51.609894Z",
     "shell.execute_reply.started": "2023-02-13T16:02:51.109748Z"
    }
   },
   "outputs": [],
   "source": [
    "sd = SimpleDiffusion(num_diffusion_timesteps=TrainingConfig.TIMESTEPS, device=\"cpu\")\n",
    "\n",
    "loader = iter(  # converting dataloader into an iterator for now.\n",
    "    get_dataloader(\n",
    "        dataset_name=BaseConfig.DATASET,\n",
    "        batch_size=6,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:55.098816Z",
     "start_time": "2023-02-03T05:53:54.709821Z"
    },
    "execution": {
     "iopub.execute_input": "2023-02-13T16:02:51.109795Z",
     "iopub.status.busy": "2023-02-13T16:02:51.108916Z",
     "iopub.status.idle": "2023-02-13T16:02:51.610820Z",
     "shell.execute_reply": "2023-02-13T16:02:51.609894Z",
     "shell.execute_reply.started": "2023-02-13T16:02:51.109748Z"
    }
   },
   "outputs": [],
   "source": [
    "x0s, _ = next(loader)\n",
    "\n",
    "noisy_images = []\n",
    "specific_timesteps = [0, 10, 50, 100, 150, 200, 250, 300, 400, 600, 800, 999]\n",
    "\n",
    "for timestep in specific_timesteps:\n",
    "    timestep = torch.as_tensor(timestep, dtype=torch.long)\n",
    "\n",
    "    xts, _ = forward_diffusion(sd, x0s, timestep)\n",
    "    xts = inverse_transform(xts) / 255.0\n",
    "    xts = make_grid(xts, nrow=1, padding=1)\n",
    "\n",
    "    noisy_images.append(xts)\n",
    "\n",
    "# Plot and see samples at different timesteps\n",
    "\n",
    "_, ax = plt.subplots(1, len(noisy_images), figsize=(10, 5), facecolor=\"white\")\n",
    "\n",
    "for i, (timestep, noisy_sample) in enumerate(zip(specific_timesteps, noisy_images)):\n",
    "    ax[i].imshow(noisy_sample.squeeze(0).permute(1, 2, 0))\n",
    "    ax[i].set_title(f\"t={timestep}\", fontsize=8)\n",
    "    ax[i].axis(\"off\")\n",
    "    ax[i].grid(False)\n",
    "\n",
    "plt.suptitle(\"Forward Diffusion Process\", y=0.9)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:55.113814Z",
     "start_time": "2023-02-03T05:53:55.100816Z"
    },
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2023-02-13T16:02:51.617227Z",
     "iopub.status.busy": "2023-02-13T16:02:51.614953Z",
     "iopub.status.idle": "2023-02-13T16:02:51.625003Z",
     "shell.execute_reply": "2023-02-13T16:02:51.623924Z",
     "shell.execute_reply.started": "2023-02-13T16:02:51.617190Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    BASE_CH = 64  # 64, 128, 256, 256\n",
    "    BASE_CH_MULT = (1, 2, 4, 4) # 32, 16, 8, 8 \n",
    "    APPLY_ATTENTION = (False, True, True, False)\n",
    "    DROPOUT_RATE = 0.1\n",
    "    TIME_EMB_MULT = 4 # 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-03T05:53:55.52238Z",
     "start_time": "2023-02-03T05:53:55.115817Z"
    },
    "code_folding": [
     0,
     21
    ],
    "execution": {
     "iopub.execute_input": "2023-02-13T16:02:51.631678Z",
     "iopub.status.busy": "2023-02-13T16:02:51.629323Z",
     "iopub.status.idle": "2023-02-13T16:02:52.173084Z",
     "shell.execute_reply": "2023-02-13T16:02:52.164905Z",
     "shell.execute_reply.started": "2023-02-13T16:02:51.631642Z"
    },
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    input_channels          = TrainingConfig.IMG_SHAPE[0],\n",
    "    output_channels         = TrainingConfig.IMG_SHAPE[0],\n",
    "    base_channels           = ModelConfig.BASE_CH,\n",
    "    base_channels_multiples = ModelConfig.BASE_CH_MULT,\n",
    "    apply_attention         = ModelConfig.APPLY_ATTENTION,\n",
    "    dropout_rate            = ModelConfig.DROPOUT_RATE,\n",
    "    time_multiple           = ModelConfig.TIME_EMB_MULT,\n",
    ")\n",
    "model.to(BaseConfig.DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=TrainingConfig.LR)\n",
    "\n",
    "dataloader = get_dataloader(\n",
    "    dataset_name  = BaseConfig.DATASET,\n",
    "    batch_size    = TrainingConfig.BATCH_SIZE,\n",
    "    device        = BaseConfig.DEVICE,\n",
    "    pin_memory    = True,\n",
    "    num_workers   = TrainingConfig.NUM_WORKERS,\n",
    ")\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "sd = SimpleDiffusion(\n",
    "    num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n",
    "    img_shape               = TrainingConfig.IMG_SHAPE,\n",
    "    device                  = BaseConfig.DEVICE,\n",
    ")\n",
    "\n",
    "scaler = amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T16:02:52.176855Z",
     "iopub.status.busy": "2023-02-13T16:02:52.175800Z",
     "iopub.status.idle": "2023-02-13T16:02:52.186599Z",
     "shell.execute_reply": "2023-02-13T16:02:52.185390Z",
     "shell.execute_reply.started": "2023-02-13T16:02:52.176811Z"
    }
   },
   "outputs": [],
   "source": [
    "total_epochs = TrainingConfig.NUM_EPOCHS + 1\n",
    "log_dir, checkpoint_dir = setup_log_directory(config=BaseConfig())\n",
    "\n",
    "generate_video = False\n",
    "ext = \".mp4\" if generate_video else \".png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T16:02:52.198421Z",
     "iopub.status.busy": "2023-02-13T16:02:52.197950Z",
     "iopub.status.idle": "2023-02-13T16:02:52.213270Z",
     "shell.execute_reply": "2023-02-13T16:02:52.212404Z",
     "shell.execute_reply.started": "2023-02-13T16:02:52.198384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Algorithm 1: Training\n",
    "\n",
    "def train_one_epoch(model, sd, loader, optimizer, scaler, loss_fn, epoch=800, \n",
    "                   base_config=BaseConfig(), training_config=TrainingConfig()):\n",
    "    \n",
    "    loss_record = MeanMetric()\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(loader), dynamic_ncols=True) as tq:\n",
    "        tq.set_description(f\"Train :: Epoch: {epoch}/{training_config.NUM_EPOCHS}\")\n",
    "         \n",
    "        for x0s, _ in loader:\n",
    "            tq.update(1)\n",
    "            \n",
    "            ts = torch.randint(low=1, high=training_config.TIMESTEPS, size=(x0s.shape[0],), device=base_config.DEVICE)\n",
    "            xts, gt_noise = forward_diffusion(sd, x0s, ts)\n",
    "\n",
    "            with amp.autocast():\n",
    "                pred_noise = model(xts, ts)\n",
    "                loss = loss_fn(gt_noise, pred_noise)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # scaler.unscale_(optimizer)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            loss_value = loss.detach().item()\n",
    "            loss_record.update(loss_value)\n",
    "\n",
    "            tq.set_postfix_str(s=f\"Loss: {loss_value:.4f}\")\n",
    "\n",
    "        mean_loss = loss_record.compute().item()\n",
    "    \n",
    "        tq.set_postfix_str(s=f\"Epoch Loss: {mean_loss:.4f}\")\n",
    "    \n",
    "    return mean_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-13T16:02:52.218438Z",
     "iopub.status.busy": "2023-02-13T16:02:52.218128Z",
     "iopub.status.idle": "2023-02-13T16:02:52.236997Z",
     "shell.execute_reply": "2023-02-13T16:02:52.236085Z",
     "shell.execute_reply.started": "2023-02-13T16:02:52.218408Z"
    }
   },
   "outputs": [],
   "source": [
    "# Algorithm 2: Sampling\n",
    "    \n",
    "@torch.no_grad()\n",
    "def reverse_diffusion(model, sd, timesteps=1000, img_shape=(3, 64, 64), \n",
    "                      num_images=5, nrow=8, device=\"cpu\", **kwargs):\n",
    "\n",
    "    x = torch.randn((num_images, *img_shape), device=device)\n",
    "    model.eval()\n",
    "\n",
    "    if kwargs.get(\"generate_video\", False):\n",
    "        outs = []\n",
    "\n",
    "    for time_step in tqdm(iterable=reversed(range(1, timesteps)), \n",
    "                          total=timesteps-1, dynamic_ncols=False, \n",
    "                          desc=\"Sampling :: \", position=0):\n",
    "\n",
    "        ts = torch.ones(num_images, dtype=torch.long, device=device) * time_step\n",
    "        z = torch.randn_like(x) if time_step > 1 else torch.zeros_like(x)\n",
    "\n",
    "        predicted_noise = model(x, ts)\n",
    "\n",
    "        beta_t                            = get(sd.beta, ts)\n",
    "        one_by_sqrt_alpha_t               = get(sd.one_by_sqrt_alpha, ts)\n",
    "        sqrt_one_minus_alpha_cumulative_t = get(sd.sqrt_one_minus_alpha_cumulative, ts) \n",
    "\n",
    "        x = (\n",
    "            one_by_sqrt_alpha_t\n",
    "            * (x - (beta_t / sqrt_one_minus_alpha_cumulative_t) * predicted_noise)\n",
    "            + torch.sqrt(beta_t) * z\n",
    "        )\n",
    "\n",
    "        if kwargs.get(\"generate_video\", False):\n",
    "            x_inv = inverse_transform(x).type(torch.uint8)\n",
    "            grid = make_grid(x_inv, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "            ndarr = torch.permute(grid, (1, 2, 0)).numpy()[:, :, ::-1]\n",
    "            outs.append(ndarr)\n",
    "\n",
    "    if kwargs.get(\"generate_video\", False): # Generate and save video of the entire reverse process. \n",
    "        frames2vid(outs, kwargs['save_path'])\n",
    "        display(Image.fromarray(outs[-1][:, :, ::-1])) # Display the image at the final timestep of the reverse process.\n",
    "        return None\n",
    "\n",
    "    else: # Display and save the image at the final timestep of the reverse process. \n",
    "        x = inverse_transform(x).type(torch.uint8)\n",
    "        grid = make_grid(x, nrow=nrow, pad_value=255.0).to(\"cpu\")\n",
    "        pil_image = TF.functional.to_pil_image(grid)\n",
    "        pil_image.save(kwargs['save_path'], format=save_path[-3:].upper())\n",
    "        display(pil_image)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-02-03T05:53:50.101Z"
    },
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-02-13T16:02:52.242828Z",
     "iopub.status.busy": "2023-02-13T16:02:52.239224Z",
     "iopub.status.idle": "2023-02-13T16:29:52.824771Z",
     "shell.execute_reply": "2023-02-13T16:29:52.823047Z",
     "shell.execute_reply.started": "2023-02-13T16:02:52.242793Z"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(1, total_epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Algorithm 1: Training\n",
    "    train_one_epoch(model, sd, dataloader, optimizer, scaler, loss_fn, epoch=epoch)\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        save_path = os.path.join(log_dir, f\"{epoch}{ext}\")\n",
    "        \n",
    "        # Algorithm 2: Sampling\n",
    "        reverse_diffusion(model, sd, timesteps=TrainingConfig.TIMESTEPS, num_images=32, generate_video=generate_video,\n",
    "            save_path=save_path, img_shape=TrainingConfig.IMG_SHAPE, device=BaseConfig.DEVICE,\n",
    "        )\n",
    "\n",
    "        # clear_output()\n",
    "        checkpoint_dict = {\n",
    "            \"opt\": optimizer.state_dict(),\n",
    "            \"scaler\": scaler.state_dict(),\n",
    "            \"model\": model.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint_dict, os.path.join(checkpoint_dir, \"ckpt.tar\"))\n",
    "        del checkpoint_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/Logs_Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    input_channels          = TrainingConfig.IMG_SHAPE[0],\n",
    "    output_channels         = TrainingConfig.IMG_SHAPE[0],\n",
    "    base_channels           = ModelConfig.BASE_CH,\n",
    "    base_channels_multiples = ModelConfig.BASE_CH_MULT,\n",
    "    apply_attention         = ModelConfig.APPLY_ATTENTION,\n",
    "    dropout_rate            = ModelConfig.DROPOUT_RATE,\n",
    "    time_multiple           = ModelConfig.TIME_EMB_MULT,\n",
    ")\n",
    "model.load_state_dict(torch.load(os.path.join(checkpoint_dir, \"ckpt.tar\"), map_location='cpu')['model'])\n",
    "\n",
    "model.to(BaseConfig.DEVICE)\n",
    "\n",
    "sd = SimpleDiffusion(\n",
    "    num_diffusion_timesteps = TrainingConfig.TIMESTEPS,\n",
    "    img_shape               = TrainingConfig.IMG_SHAPE,\n",
    "    device                  = BaseConfig.DEVICE,\n",
    ")\n",
    "\n",
    "log_dir = \"inference_results\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video = True\n",
    "\n",
    "ext = \".mp4\" if generate_video else \".png\"\n",
    "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "reverse_diffusion(\n",
    "    model,\n",
    "    sd,\n",
    "    num_images=256,\n",
    "    generate_video=generate_video,\n",
    "    save_path=save_path,\n",
    "    timesteps=1000,\n",
    "    img_shape=TrainingConfig.IMG_SHAPE,\n",
    "    device=BaseConfig.DEVICE,\n",
    "    nrow=32,\n",
    ")\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video = False\n",
    "\n",
    "ext = \".mp4\" if generate_video else \".png\"\n",
    "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "\n",
    "reverse_diffusion(\n",
    "    model,\n",
    "    sd,\n",
    "    num_images=128,\n",
    "    generate_video=generate_video,\n",
    "    save_path=save_path,\n",
    "    timesteps=1000,\n",
    "    img_shape=TrainingConfig.IMG_SHAPE,\n",
    "    device=BaseConfig.DEVICE,\n",
    "    nrow=8,\n",
    ")\n",
    "\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video = False\n",
    "\n",
    "ext = \".mp4\" if generate_video else \".png\"\n",
    "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "\n",
    "reverse_diffusion(\n",
    "    model,\n",
    "    sd,\n",
    "    num_images=128,\n",
    "    generate_video=generate_video,\n",
    "    save_path=save_path,\n",
    "    timesteps=1000,\n",
    "    img_shape=TrainingConfig.IMG_SHAPE,\n",
    "    device=BaseConfig.DEVICE,\n",
    "    nrow=8,\n",
    ")\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video = False\n",
    "\n",
    "ext = \".mp4\" if generate_video else \".png\"\n",
    "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "\n",
    "reverse_diffusion(\n",
    "    model,\n",
    "    sd,\n",
    "    num_images=128,\n",
    "    generate_video=generate_video,\n",
    "    save_path=save_path,\n",
    "    timesteps=1000,\n",
    "    img_shape=TrainingConfig.IMG_SHAPE,\n",
    "    device=BaseConfig.DEVICE,\n",
    "    nrow=8,\n",
    ")\n",
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video = False\n",
    "\n",
    "ext = \".mp4\" if generate_video else \".png\"\n",
    "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "\n",
    "reverse_diffusion(\n",
    "    model,\n",
    "    sd,\n",
    "    num_images=128,\n",
    "    generate_video=generate_video,\n",
    "    save_path=save_path,\n",
    "    timesteps=1000,\n",
    "    img_shape=TrainingConfig.IMG_SHAPE,\n",
    "    device=BaseConfig.DEVICE,\n",
    "    nrow=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video = False\n",
    "\n",
    "ext = \".mp4\" if generate_video else \".png\"\n",
    "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "\n",
    "reverse_diffusion(\n",
    "    model,\n",
    "    sd,\n",
    "    num_images=128,\n",
    "    generate_video=generate_video,\n",
    "    save_path=save_path,\n",
    "    timesteps=1000,\n",
    "    img_shape=TrainingConfig.IMG_SHAPE,\n",
    "    device=BaseConfig.DEVICE,\n",
    "    nrow=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_gif = False\n",
    "\n",
    "ext = \".gif\" if generate_gif else \".png\"\n",
    "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "\n",
    "reverse_diffusion(\n",
    "    model,\n",
    "    sd,\n",
    "    num_images=32,\n",
    "    generate_gif=generate_gif,\n",
    "    save_path=save_path,\n",
    "    timsteps=TrainingConfig.TIMESTEPS,\n",
    "    img_shape=TrainingConfig.IMG_SHAPE,\n",
    "    device=BaseConfig.DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_gif = False\n",
    "\n",
    "ext = \".gif\" if generate_gif else \".png\"\n",
    "filename = f\"{datetime.now().strftime('%Y%m%d-%H%M%S')}{ext}\"\n",
    "\n",
    "save_path = os.path.join(log_dir, filename)\n",
    "\n",
    "\n",
    "reverse_diffusion(\n",
    "    model,\n",
    "    sd,\n",
    "    num_images=32,\n",
    "    generate_gif=generate_gif,\n",
    "    save_path=save_path,\n",
    "    timsteps=TrainingConfig.TIMESTEPS,\n",
    "    img_shape=TrainingConfig.IMG_SHAPE,\n",
    "    device=BaseConfig.DEVICE,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
